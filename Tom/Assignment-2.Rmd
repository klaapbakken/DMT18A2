---
title: "Kaggle-Expedia-Analysis"
author: "T.W Battaglia"
date: "April 26, 2018"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
library(tidyverse)
library(readr)
library(lubridate)
library(scales)
library(viridis)
library(gbm)
library(randomForest)

# Load training data
load("../data/training_processed.rda")
```

### Data Import
Code below is presented only for reproducibility. The processed data is located within the R-object `training_processed.rda`. The command `load("training_processed.rda")` will load the object into the environment.

#### Import training data
```{r, eval=FALSE}
# Import using readr (faster)
training = read_csv('/Users/tom/GoogleDrive/School/Period-5/Data-Mining/Assignments/Assignment-2/Data Mining VU data/training_set_VU_DM_2014.csv', na = "NULL")

# View column type summary
glimpse(training)
```

#### Expand the data_time feature
```{r, eval=F}
# Expand the data-time into year, month, day, weekday
training = training %>% 
  separate(date_time, c("ymd", "hour"), sep = " ", remove = F) %>%
  select(-hour) %>% 
  mutate(ymd = ymd(ymd),
         wday = wday(date_time, label = TRUE),
         month = month(date_time, label = TRUE),
         week = week(date_time),
         day = day(date_time))
```

#### View column types
```{r, eval = F}
# View column types summary after column changes
glimpse(training)
```

#### Save table as r-object for faster import
```{r, eval = F}
# Save files as .rda objects
save(training, file = "training_processed.rda")
```


### Get Summary Statistics

#### Basic dimensions
```{r}
# View dimensions
dim(training)
```

#### Number of unique searchs
Large amount of unique searchs to deal with.
```{r}
# Number of unique searches
training %>% 
  group_by(srch_id) %>% 
  tally() %>% 
  nrow()
```

#### Span of the bookings within the training data
Data does not include information over the entire year.
```{r}
# Differences between min and max days
max(training$ymd) - min(training$ymd) 
```

#### Distribution of length of stay
A majority of the searches have a short length of stay
```{r}
# Length of stay distribution
training %>% 
  group_by(srch_length_of_stay) %>% 
  tally() %>% 
  top_n(20) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = srch_length_of_stay, y = n / 4958347)) +
  geom_bar(stat = "identity") +
  theme_bw(base_size = 14) +
  scale_y_continuous(labels = scales::percent) +
  xlab("Length of stay (days)") +
  ylab("Fraction of total searches")
```

#### Number of clicks & bookings
Data set is very imbalanced with many more searches that were not clicked or not booked.
```{r}
# Number of clicks & bookings == TRUE
training %>% 
  group_by(click_bool, booking_bool) %>% 
  tally() %>% 
  ungroup() %>% 
  mutate(Percent = (n / sum(n)) * 100)
```

#### Booking and Clicking instances 
There are about 
```{r}
# Find the srch_ids click/book rates 
srch_id_none = training %>% 
  group_by(srch_id) %>% 
  summarise(Total_query = length(srch_id),
            Click_count = sum(click_bool),
            Book_count = sum(booking_bool)) %>% 
  arrange(desc(Click_count))

head(srch_id_none)
```


#### Number of properties
There are a large amount of unique properties within the dataset, but a fairly equal representation of each property.
```{r}
# Number of properties
## 129,113 different properties
## Top property has 2357 searches
training %>% 
  group_by(prop_id) %>% 
  tally() %>% 
  top_n(20) %>% 
  arrange(desc(n))
```

#### Number of sites
There is a dominating website/search portal that has a large amount of the searches. I assume this is expedia.com.
```{r}
# Number of sites to book from
## 34 different sites
training %>% 
  group_by(site_id) %>% 
  tally() %>% 
  top_n(20) %>% 
  arrange(desc(n))
```

#### Number of destinations
There are some destinations that appear more frequently. This could be major cities, like NYC or Chicago.
```{r}
# Number of destinations
## 18,127 different destinations
training %>% 
  group_by(srch_destination_id) %>% 
  tally() %>% 
  top_n(20) %>% 
  arrange(desc(n))
```

#### Origins + Destinations
Many of the origin and destinations are within the same country. I assume county code 219 is U.S.A.
```{r}
# Origin + Destination
training %>% 
  group_by(visitor_location_country_id, prop_country_id) %>% 
  tally() %>% 
  ungroup() %>% 
  top_n(20) %>% 
  arrange(desc(n))
```

#### Number of missing values percentages
Many of the missing values come from the competitor values. This must be dealt with later.
```{r}
# Gather the amount of missing values in each column
missing_values = training %>% 
  summarise_all(funs(100 * mean(is.na(.) ))) %>% 
  gather(Variable, Value) %>% 
  arrange(desc(Value)) %>% 
  mutate(Variable = as_factor(Variable))

# Write table to disk
write.csv(missing_values, "missing_values.csv")

# Simple dotplot of missing values percentages
missing_values %>% 
  filter(Value > 0) %>% 
  ggplot(aes(x = Variable, y = Value)) + 
  geom_point(stat = 'identity', fill = "black", size = 3.5, alpha = 0.60) +
  geom_segment(aes(y = 0, 
                   x = Variable, 
                   yend = Value, 
                   xend = Variable), 
               color = "black") +
  coord_flip() +
  xlab("") + ylab("Missing values (percent)") +
  theme_bw() +
  geom_hline(yintercept = 50, color = "red", alpha = 0.45) +
  ggtitle("Percentage of missing observations within each feature")
```

### Properties of clicked hotels

#### Subset to view only clicked hotels
```{r}
# Subset to view only clicked hotels
training_click = training %>% 
  filter(click_bool == 1)

# Get dimensions
dim(training_click)
```


### Properties of booked hotels

#### Subset to view only booked hotels
```{r}
# Subset to view only booked hotels
training_booked = training %>% 
  filter(booking_bool == 1)

# Get dimensions
dim(training_booked)
```

#### View booking-rates over seasonality
```{r}
# Count booked occurances per day
training_booked %>% 
  group_by(ymd) %>% 
  tally() %>% 
  mutate(Fraction = n / 138390) %>% 
  ggplot(aes(x = ymd, y = Fraction)) +
  geom_line() +
  theme_bw(base_size = 14) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  xlab("Month") +
  ylab("Fraction of bookings") +
  ggtitle("Fraction of total bookings, by month")
```

#### View booking-rates by week
```{r}
# Count booked occurances per week
training_booked %>% 
  group_by(wday) %>% 
  tally() %>% 
  mutate(Fraction = n / 138390) %>% 
  ggplot(aes(x = wday, y = Fraction, group = 1)) +
  geom_line() +
  theme_bw(base_size = 14) +
  xlab("Weekday") +
  ylab("Fraction of bookings") +
  ggtitle("Fraction of total bookings, by week day")
```

#### View booking-rates by week and seasonality
```{r}
# Count booked occurances per day
training_booked %>% 
  group_by(month, wday) %>% 
  tally() %>% 
  mutate(Fraction = (n / 138390) * 100) %>% 
  ggplot(aes(x = month, y = wday, fill = Fraction)) +
  geom_tile() +
  theme_bw(base_size = 14) +
  scale_fill_viridis() + 
  ggtitle("Fraction of total bookings, by month and weekday")
```



### Gather data from single user
```{r}
# Subset to search query 1
single_user = training %>% 
  filter(srch_id == 14059)

# View table
head(single_user)
```


### Fix missing values
https://ajourneyintodatascience.quora.com/Learning-to-Rank-Personalize-Expedia-Hotel-Searches-ICDM-2013-Data-Cleaning

#### Set the competitor values to 0 (comp_XXXX)
This assumes there is no major difference between the competitor price and Expedia price.
```{r}
training_noNA = training %>% 
  replace_na(list(comp1_inv = 0,
                  comp2_inv = 0,
                  comp2_inv = 0,
                  comp3_inv = 0,
                  comp4_inv = 0,
                  comp5_inv = 0,
                  comp6_inv = 0,
                  comp7_inv = 0,
                  comp8_inv = 0,
                  comp1_rate = 0,
                  comp2_rate = 0,
                  comp3_rate = 0,
                  comp4_rate = 0,
                  comp5_rate = 0,
                  comp6_rate = 0,
                  comp7_rate = 0,
                  comp8_rate = 0,
                  comp1_rate_percent_diff = 0,
                  comp2_rate_percent_diff = 0,
                  comp3_rate_percent_diff = 0,
                  comp4_rate_percent_diff = 0,
                  comp5_rate_percent_diff = 0,
                  comp6_rate_percent_diff = 0,
                  comp7_rate_percent_diff = 0,
                  comp8_rate_percent_diff = 0))
```

#### Set the hotel reviews to -1.
This assumes the worst for the hotel's that are not rated.
```{r}
training_noNA = training_noNA %>% 
  replace_na(list(prop_review_score = -1,
                  prop_starrating = -1)) 
```

#### Set the location score #2 to the minimum value across all scores
```{r}
training_noNA = training_noNA %>% 
  group_by(srch_destination_id) %>% 
  replace_na(list(prop_location_score2 = min(.$prop_location_score2, na.rm = T))) %>% 
  ungroup()
```

#### Remove gross_bookings_usd
This feature is not in the testing set.
```{r}
training_noNA = training_noNA %>% 
  select(-gross_bookings_usd)
```

#### Set the distance from visitor to hotel as median across all values
```{r}
training_noNA = training_noNA %>% 
  replace_na(list(orig_destination_distance = median(.$orig_destination_distance, 
                                                     na.rm = T)))
```

#### Set the search query affinity (probability) to 0
```{r}
training_noNA = training_noNA %>% 
  replace_na(list(srch_query_affinity_score = 0))
```


#### Within each searchID, 

### Feature Engineering

#### visitor_new_bool : visitor is a new customer (Bool)
```{r}
training_noNA = training_noNA %>% 
  mutate(visitor_new_bool = if_else(is.na(visitor_hist_starrating), 1, 0))
```

#### srch_booking_date : current date + time to book (Date)
```{r}
training_noNA = training_noNA %>% 
  mutate(srch_booking_date = ymd + srch_booking_window)
```

#### srch_holiday_bool : is the booking day on any known holiday of 5-7 window (Bool)
```{r}

```

#### srch_people_num : total people on trip (adult + children) (Integer)
```{r}
training_noNA = training_noNA %>% 
  mutate(srch_people_num = srch_adults_count + srch_children_count)
```

#### srch_children_bool : are there any children present (Bool)
```{r}
training_noNA = training_noNA %>% 
  mutate(srch_children_bool = if_else(srch_children_count > 0, 1, 0))
```

#### price_order : ordering of the price within a srch)id (Ordered Integer)


### Subsample training data

#### Sample srch_id's
```{r}
# Number of srch id's to gather
length(levels(as.factor(training$srch_id))) * 0.10

# Gather 20% of training queries
subsample_idx = sample(levels(as.factor(training$srch_id)), size = 19980)

# Keep only sampled queries
training_sampled = training %>% 
  filter(srch_id %in% subsample_idx)
```

#### Verify accurate representation
```{r}
# Number of clicks & bookings == TRUE
training_sampled %>% 
  group_by(click_bool, booking_bool) %>% 
  tally() %>% 
  ungroup() %>% 
  mutate(Percent = (n / sum(n)) * 100)
```

